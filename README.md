# Finetuning-of-llm
multi modal

# ğŸ”§ Fine-Tuning IDEFICS 9B on PokÃ©mon Go Cards

This project demonstrates fine-tuning the **IDEFICS 9B** vision-language model on a custom **PokÃ©mon Go card dataset** to enhance its performance in **visual question answering (VQA)**.

---

## ğŸ§  Why Fine-Tune?

The base IDEFICS model is trained for general multimodal tasks. Fine-tuning it on PokÃ©mon Go cards helps the model:

- Better understand **card-specific layouts and designs**
- Accurately answer **questions related to card content**
- Adapt to **domain-specific vocabulary** and formats

---

## ğŸ› ï¸ How It Was Done

- âœ… **Model**: [IDEFICS 9B](https://huggingface.co/HuggingFaceM4/idefics-9b)
- âš™ï¸ **Fine-Tuning Method**: [PEFT (LoRA)](https://github.com/huggingface/peft)
- ğŸ“‰ **Quantization**: 4-bit using `bitsandbytes`
- ğŸ’¾ **Platform**: Google Colab
- ğŸ“š **Frameworks**:
  - Hugging Face Transformers
  - PEFT
  - Datasets
  - Torch + CUDA

---

## ğŸ“ Dataset Format

Each example includes:
```json
{
  "image": "path/to/image.jpg",
  "question": "What is the PokÃ©mon's attack power?",
  "answer": "120"
}
